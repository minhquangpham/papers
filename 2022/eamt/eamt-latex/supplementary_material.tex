% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{emnlp2021}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\renewcommand{\UrlFont}{\ttfamily\small}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{multirow}
\usepackage{xcolor,colortbl}
\usepackage{amsmath}
%\usepackage{fdsymbol}
\usepackage{amssymb}
\usepackage{makecell}
\usepackage[super]{nth}
\usepackage{arydshln}
\usepackage{algorithm,algorithmic}
%\usepackage[noend]{algpseudocode}
\usepackage{regexpatch}
\usepackage{subcaption}
% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\usepackage{url}
\usepackage{longtable}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage[draft]{todo}
\usepackage[normalem]{ulem}
\usepackage{xspace}
\usepackage{float}

\usepackage{pgfplots}
\usepackage{pgfplotstable}

\let\oldbibitem\bibitem
\def\bibitem{\vfill\oldbibitem}

\usepackage{soulutf8}
\usepackage{tabularx}

\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{caption}
\usepackage{subcaption}

\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyFuture}[1]{\done[FY]\Todo[FY:]{\textcolor{red}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}

\newcommand{\revision}[1]{\textcolor{red}{#1}}
\newcommand{\revisiondel}[1]{}
\newcommand{\src}{\ensuremath{\mathbf{f}}} % source sentence
\newcommand{\trg}{\ensuremath{\mathbf{e}}} % target sentence
\newcommand{\domain}[1]{\texttt{\textsc{#1}}}
\newcommand{\system}[1]{\texttt{{#1}}}

\newcommand{\vlambda}{\ensuremath{\boldsymbol\lambda}\xspace} % parameters vector for a distribution
\newcommand{\vtheta}{\ensuremath{\boldsymbol\theta}\xspace} % parameters vector for a distribution
\newcommand{\vpsi}{\ensuremath{\boldsymbol\psi}\xspace} % parameters vector for a distribution
\newcommand{\indic}[1]{\ensuremath{\mathbb{I}(#1)}}
% \newcommand{\SB}[1]{\textcolor{green}{#1}}
% \newcommand{\SW}[1]{\textcolor{red}{#1}}
\newcommand{\SB}[1]{\textbf{#1}}
\newcommand{\SW}[1]{\underline{#1}}
% limits underneath
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand\textfraction{.1}
\renewcommand\floatpagefraction{.95}
\newcommand{\sbcl}[2]{{\scriptsize #1 \hfill $|$ \hfill  #2}}
\usepackage{multirow}
\usepackage{adjustbox}

\makeatletter
\newenvironment{breakablealgorithm}
  {% \begin{breakablealgorithm}
   \begin{center}
     \refstepcounter{algorithm}% New algorithm
     \hrule height.8pt depth0pt \kern2pt% \@fs@pre for \@fs@ruled
     \renewcommand{\caption}[2][\relax]{% Make a new \caption
       {\raggedright\textbf{\fname@algorithm~\thealgorithm} ##2\par}%
       \ifx\relax##1\relax % #1 is \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##2}%
       \else % #1 is not \relax
         \addcontentsline{loa}{algorithm}{\protect\numberline{\thealgorithm}##1}%
       \fi
       \kern2pt\hrule\kern2pt
     }
  }{% \end{breakablealgorithm}
     \kern2pt\hrule\relax% \@fs@post for \@fs@ruled
   \end{center}
  }
\makeatother

\title{Supplementary material for paper ``Multi-Domain Adaptation in Neural Machine Translation with Dynamic Sampling Strategies''}

\date{}

\begin{document}
\maketitle
\setlength{\abovedisplayskip}{2pt}
\setlength{\belowdisplayskip}{2pt}

\section*{A - Generalized Multi-Domain Dynamic Adaptation Curriculum Algorithm}
The pseudo-code for the generalized Multi-Domain Adaptation Dynamic Sampling Algorithm  is in algorithm~\ref{alg:mdmt}.
\section*{B - Experiments with automatic domains}
\begin{table*}[htbp]
  \centering
  \footnotesize
  \begin{tabular}{|p{0.5cm}|*{7}{c|}|*{6}{c|}} 
  \hline
\multirow{2}*{Cl. }&\multirow{2}*{size}&\multicolumn{6}{c||}{Domain content}&\multicolumn{6}{c|}{MDAC systems}\\
  \cline{3-14}
&&MED&ECB&IT&LAW&REL&TALK&MED&ECB&IT&LAW&REL&TALK \\
\hline
1&27436&0.24&0.11&0.47&0.17&0.00&0.01&0.01&0.00&0.01&0.00&0.00&0.01 \\
2&68108&0.48&0.04&0.19&0.28&0.00&0.00&0.02&0.03&0.02&0.01&0.02&0.02 \\
3&182594&1.00&0.00&0.00&0.00&0.00&0.00&0.03&0.06&0.04&0.06&0.03&0.03 \\
4&323474&0.05&0.06&0.01&0.87&0.00&0.00&0.03&\underline{0.07}&0.03&\underline{0.20}&0.03&0.04 \\
5&137451&0.03&0.00&0.00&0.00&0.86&0.10&0.03&0.04&0.04&0.06&\underline{0.17}&0.04 \\
6&109949&0.44&0.04&0.40&0.07&0.01&0.04&0.04&0.06&0.04&0.05&0.02&0.03 \\
7&133395&0.92&0.01&0.01&0.05&0.00&0.00&0.03&0.04&0.03&0.05&0.05&0.03 \\
8&91464&0.98&0.00&0.00&0.02&0.00&0.00&0.04&0.03&0.04&0.04&0.05&0.03 \\
9&61353&0.02&0.01&0.96&0.01&0.00&0.00&0.03&0.02&0.03&0.01&0.04&0.02 \\
10&301639&0.98&0.00&0.00&0.02&0.00&0.00&0.02&0.01&0.01&0.01&0.02&0.03 \\
11&225347&0.93&0.00&0.01&0.04&0.00&0.01&0.03&0.04&0.04&0.02&0.03&0.04 \\
12&92982&0.98&0.00&0.01&0.01&0.00&0.00&0.04&0.02&0.03&0.01&0.03&0.03 \\
13&356377&0.99&0.00&0.00&0.01&0.00&0.00&0.03&0.03&0.04&0.03&0.03&0.04 \\
14&17260&0.03&0.37&0.01&0.59&0.00&0.00&0.03&0.03&0.03&0.02&0.03&0.02 \\
15&333957&0.98&0.00&0.01&0.01&0.00&0.00&0.04&0.02&0.03&0.01&0.03&0.03 \\
16&136944&0.02&0.89&0.01&0.08&0.00&0.00&0.04&\underline{0.08}&0.03&0.04&0.03&0.04 \\
17&30443&0.96&0.01&0.02&0.01&0.00&0.00&0.04&0.03&0.03&0.03&0.04&0.03 \\
18&54378&0.93&0.00&0.05&0.02&0.00&0.00&0.04&0.04&0.02&0.03&0.07&0.03 \\
19&245000&0.99&0.00&0.00&0.00&0.00&0.00&0.04&0.04&0.03&0.03&0.04&0.03 \\
20&27227&0.15&0.00&0.79&0.05&0.00&0.01&0.04&0.03&0.03&0.02&0.04&0.03 \\
21&182990&0.99&0.00&0.00&0.01&0.00&0.00&0.03&0.02&0.03&0.03&0.03&0.03 \\
22&222802&0.21&0.01&0.40&0.04&0.02&0.32&0.04&0.04&\underline{0.08}&0.02&0.01&\underline{0.08} \\
23&27534&0.11&0.48&0.02&0.39&0.00&0.00&0.03&0.03&0.05&0.02&0.01&0.04 \\
24&47065&0.99&0.00&0.01&0.00&0.00&0.00&0.04&0.02&0.05&0.05&0.03&0.04 \\
25&8129&0.95&0.00&0.04&0.01&0.00&0.00&0.03&0.03&0.04&0.03&0.02&0.02 \\
26&28237&0.59&0.02&0.23&0.11&0.00&0.05&0.03&0.02&0.02&0.01&0.03&0.02 \\
27&134828&0.53&0.00&0.02&0.00&0.01&0.44&0.04&0.03&0.03&0.01&0.01&0.05 \\
28&109324&0.99&0.00&0.00&0.01&0.00&0.00&0.04&0.03&0.03&0.02&0.01&0.02 \\
29&25561&0.56&0.16&0.08&0.16&0.00&0.04&0.03&0.03&0.03&0.02&0.02&0.03 \\
30&109260&0.01&0.06&0.00&0.92&0.00&0.00&0.03&0.03&0.03&0.06&0.04&0.04 \\
\hline
\end{tabular}
\caption{Automatic clustering experiments. We report the size of each cluster. In the 6 left columns, each line gives the proportions of the domains in each cluster. In the 6 right columns, each column corresponds to a MDAC experiment; each line gives the cumulated proportion of the corresponding cluster in the training data. For instance, when targeting the domain \domain{ecb}, cluster~4 (mostly \domain{law}) is sampled with a probability of $0.07$, and cluster~16 (mostly \domain{ecb}) is sampled with probability $0.08$. For each system, we underline the most often sampled clusters.}
\label{tab:automatic_domains}
\end{table*}

This experiment aims to simulate with automatic domains a scenario where the number of ``domains'' is large and where some ``domains'' are close and can effectively share information. Full results are in Table~\ref{tab:automatic_domains}. Cluster sizes vary from approximately 8k sentences (cluster~24) up to more than 350k sentences. More than 2/3 of these clusters mostly comprise texts from one single domain, as for cluster 12, which is predominantly \domain{med}, the remaining clusters typically mix 2 domains. According to the table, for each system, the clusters most sampled by MDAC contain most of the data of the corresponding domain. This demonstrates that MDAC is able to find related data to the task automatically. However, MDAC performance is still far behind that of the supervised scenario, as we explain in the paper. 
\\
\\
\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{breakablealgorithm}
\caption{Multi-Domain Adaptation Dynamic Sampling} \label{alg:mdmt}
\begin{algorithmic}[1]
\REQUIRE {
  \begin{itemize}
    \setlength{\itemsep}{-1pt}    \setlength{\parsep}{0pt}
  \item[] 
  \item $n_d$ corpora $C^d, d \in [1, \dots,n_d]$ for $n_d$ domains equipped by an empirical distribution $D_d(x)$
  \item $n_d$  dev sets $Dev^d, d\in [1, \dots,n_d]$ for $n_d$ domains.
  \item Domain testing distribution $\vlambda^t \in \mathbb{R}^{n_d}$
  \item Batch size $B$
  \item Domain Dynamic Sampling Distribution $\lambda^l_i \in \mathbb{R}^{n_d}$ where $i$ indexes iterations.
  \item $Eval\_scores = []$
  \item $Early\_stopping$ criterion
  \item Total training iterations $iter\_num$
  \item Update rule for sampling distribution $\vlambda^l_i$
 \end{itemize}}
\REPEAT 
\STATE{// Start of iteration i}
\STATE{Randomly pick $d \in [1,\dots,n_d]$ from sampling distribution $\lambda^l_i$}
\STATE{Sample $B$ sentences from $C^d$ with empirical distribution $D_d(x)$}
\STATE{Update model by applying SGD computed from $B$ sampled sentences}
\IF{$i \equiv 0 \mod{eval\_step}$}
	\STATE{Evaluate current model with $n_d$ dev sets. $S^d_i$ is the performance at iteration $i^{th}$ on domain $d$}
	\STATE{Report weighted score using test distribution $\vlambda^t$. $$eval(i) = \displaystyle{\mathop{\sum}_d^{n_d} \lambda^t(d) S^d_i}$$}
	\STATE{$Eval\_scores.append(eval(i))$}
\ENDIF
\IF{$i \equiv 0 \mod{sampler\_updating\_step}$}
	\STATE Update $\vlambda^l_i$
\ENDIF
\IF{$Early\_stopping(Eval\_scores)$}
	\STATE{break}
\ENDIF
\STATE{$i=i+1$}
\UNTIL{$i> iter\_num$}
\end{algorithmic}
\end{breakablealgorithm}

\end{document}






