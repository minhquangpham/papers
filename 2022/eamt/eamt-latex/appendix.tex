\section*{A - Generalized Multi-Domain Dynamic Adaptation Curriculum Algorithm}
The pseudo-code for the generalized Multi-Domain Adaptation Dynamic Sampling Algorithm  is in algorithm~\ref{alg:mdmt}.
\begin{breakablealgorithm}
\caption{Multi-Domain Adaptation Dynamic Sampling} \label{alg:mdmt}
\begin{algorithmic}[1]
\REQUIRE {
  \begin{itemize}
    \setlength{\itemsep}{-1pt}    \setlength{\parsep}{0pt}
  \item[] 
  \item $n_d$ corpora $C^d, d \in [1, \dots,n_d]$ for $n_d$ domains equipped by an empirical distribution $D_d(x)$
  \item $n_d$  dev sets $Dev^d, d\in [1, \dots,n_d]$ for $n_d$ domains.
  \item Domain testing distribution $\vlambda^t \in \mathbb{R}^{n_d}$
  \item Batch size $B$
  \item Domain Dynamic Sampling Distribution $\lambda^l_i \in \mathbb{R}^{n_d}$ where $i$ indexes iterations.
  \item $Eval\_scores = []$
  \item $Early\_stopping$ criterion
  \item Total training iterations $iter\_num$
  \item Update rule for sampling distribution $\vlambda^l_i$
 \end{itemize}}
\REPEAT 
\STATE{// Start of iteration i}
\STATE{Randomly pick $d \in [1,\dots,n_d]$ from sampling distribution $\lambda^l_i$}
\STATE{Sample $B$ sentences from $C^d$ with empirical distribution $D_d(x)$}
\STATE{Update model by applying SGD computed from $B$ sampled sentences}
\IF{$i \equiv 0 \mod{eval\_step}$}
	\STATE{Evaluate current model with $n_d$ dev sets. $S^d_i$ is the performance at iteration $i^{th}$ on domain $d$}
	\STATE{Report weighted score using test distribution $\vlambda^t$. $$eval(i) = \displaystyle{\mathop{\sum}_d^{n_d} \lambda^t(d) S^d_i}$$}
	\STATE{$Eval\_scores.append(eval(i))$}
\ENDIF
\IF{$i \equiv 0 \mod{sampler\_updating\_step}$}
	\STATE Update $\vlambda^l_i$
\ENDIF
\IF{$Early\_stopping(Eval\_scores)$}
	\STATE{break}
\ENDIF
\STATE{$i=i+1$}
\UNTIL{$i> iter\_num$}
\end{algorithmic}
\end{breakablealgorithm}
\section*{B - Experiments with automatic domains}
\begin{table*}[htbp]
  \centering
  \footnotesize
  \begin{tabular}{|p{0.5cm}|*{7}{c|}|*{6}{c|}} 
  \hline
\multirow{2}*{Cl. }&\multirow{2}*{size}&\multicolumn{6}{c||}{Domain content}&\multicolumn{6}{c|}{MDAC systems}\\
  \cline{3-14}
&&MED&ECB&IT&LAW&REL&TALK&MED&ECB&IT&LAW&REL&TALK \\
\hline
1&27436&0.24&0.11&0.47&0.17&0.00&0.01&0.01&0.00&0.01&0.00&0.00&0.01 \\
2&68108&0.48&0.04&0.19&0.28&0.00&0.00&0.02&0.03&0.02&0.01&0.02&0.02 \\
3&182594&1.00&0.00&0.00&0.00&0.00&0.00&0.03&0.06&0.04&0.06&0.03&0.03 \\
4&323474&0.05&0.06&0.01&0.87&0.00&0.00&0.03&\underline{0.07}&0.03&\underline{0.20}&0.03&0.04 \\
5&137451&0.03&0.00&0.00&0.00&0.86&0.10&0.03&0.04&0.04&0.06&\underline{0.17}&0.04 \\
6&109949&0.44&0.04&0.40&0.07&0.01&0.04&0.04&0.06&0.04&0.05&0.02&0.03 \\
7&133395&0.92&0.01&0.01&0.05&0.00&0.00&0.03&0.04&0.03&0.05&0.05&0.03 \\
8&91464&0.98&0.00&0.00&0.02&0.00&0.00&0.04&0.03&0.04&0.04&0.05&0.03 \\
9&61353&0.02&0.01&0.96&0.01&0.00&0.00&0.03&0.02&0.03&0.01&0.04&0.02 \\
10&301639&0.98&0.00&0.00&0.02&0.00&0.00&0.02&0.01&0.01&0.01&0.02&0.03 \\
11&225347&0.93&0.00&0.01&0.04&0.00&0.01&0.03&0.04&0.04&0.02&0.03&0.04 \\
12&92982&0.98&0.00&0.01&0.01&0.00&0.00&0.04&0.02&0.03&0.01&0.03&0.03 \\
13&356377&0.99&0.00&0.00&0.01&0.00&0.00&0.03&0.03&0.04&0.03&0.03&0.04 \\
14&17260&0.03&0.37&0.01&0.59&0.00&0.00&0.03&0.03&0.03&0.02&0.03&0.02 \\
15&333957&0.98&0.00&0.01&0.01&0.00&0.00&0.04&0.02&0.03&0.01&0.03&0.03 \\
16&136944&0.02&0.89&0.01&0.08&0.00&0.00&0.04&\underline{0.08}&0.03&0.04&0.03&0.04 \\
17&30443&0.96&0.01&0.02&0.01&0.00&0.00&0.04&0.03&0.03&0.03&0.04&0.03 \\
18&54378&0.93&0.00&0.05&0.02&0.00&0.00&0.04&0.04&0.02&0.03&0.07&0.03 \\
19&245000&0.99&0.00&0.00&0.00&0.00&0.00&0.04&0.04&0.03&0.03&0.04&0.03 \\
20&27227&0.15&0.00&0.79&0.05&0.00&0.01&0.04&0.03&0.03&0.02&0.04&0.03 \\
21&182990&0.99&0.00&0.00&0.01&0.00&0.00&0.03&0.02&0.03&0.03&0.03&0.03 \\
22&222802&0.21&0.01&0.40&0.04&0.02&0.32&0.04&0.04&\underline{0.08}&0.02&0.01&\underline{0.08} \\
23&27534&0.11&0.48&0.02&0.39&0.00&0.00&0.03&0.03&0.05&0.02&0.01&0.04 \\
24&47065&0.99&0.00&0.01&0.00&0.00&0.00&0.04&0.02&0.05&0.05&0.03&0.04 \\
25&8129&0.95&0.00&0.04&0.01&0.00&0.00&0.03&0.03&0.04&0.03&0.02&0.02 \\
26&28237&0.59&0.02&0.23&0.11&0.00&0.05&0.03&0.02&0.02&0.01&0.03&0.02 \\
27&134828&0.53&0.00&0.02&0.00&0.01&0.44&0.04&0.03&0.03&0.01&0.01&0.05 \\
28&109324&0.99&0.00&0.00&0.01&0.00&0.00&0.04&0.03&0.03&0.02&0.01&0.02 \\
29&25561&0.56&0.16&0.08&0.16&0.00&0.04&0.03&0.03&0.03&0.02&0.02&0.03 \\
30&109260&0.01&0.06&0.00&0.92&0.00&0.00&0.03&0.03&0.03&0.06&0.04&0.04 \\
\hline
\end{tabular}
\caption{Automatic clustering experiments. We report the size of each cluster. In the 6 left columns, each line gives the proportions of the domains in each cluster. In the 6 right columns, each column corresponds to a MDAC experiment; each line gives the cumulated proportion of the corresponding cluster in the training data. For instance, when targeting the domain \domain{ecb}, cluster~4 (mostly \domain{law}) is sampled with a probability of $0.07$, and cluster~16 (mostly \domain{ecb}) is sampled with probability $0.08$. For each system, we underline the most often sampled clusters.}
\label{tab:automatic_domains}
\end{table*}

This experiment aims to simulate with automatic domains a scenario where the number of ``domains'' is large and where some ``domains'' are close and can effectively share information. Full results are in Table~\ref{tab:automatic_domains}. Cluster sizes vary from approximately 8k sentences (cluster~24) up to more than 350k sentences. More than 2/3 of these clusters mostly comprise texts from one single domain, as for cluster 12, which is predominantly \domain{med}, the remaining clusters typically mix 2 domains. According to the table, for each system, the clusters most sampled by MDAC contain most of the data of the corresponding domain. This demonstrates that MDAC is able to find related data to the task automatically. However, MDAC performance is still far behind that of the supervised scenario, as we explain in the paper. 
\\
\\
\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

