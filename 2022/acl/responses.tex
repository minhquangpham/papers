\documentclass[12pt,times,a4paper,twoside]{article}

\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage[left=3cm,right=3cm,top=2cm,bottom=3cm]{geometry}
\usepackage{pagenote}

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{mathtools,xparse}
\usepackage{mathrsfs}

\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
% \usepackage{color,soul}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
% \usepackage{xcolor} -- implied by xcolor

\usepackage{listings}
\usepackage{spverbatim}
\usepackage{fancyvrb}
\usepackage{hyperref}
\usepackage{float}
\usepackage{natbib}

\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{soulutf8}
\usepackage{tabularx}
\usepackage{url}

\usepackage[draft]{todo}
\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}

\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\setlength{\belowdisplayskip}{8pt} \setlength{\belowdisplayshortskip}{8pt}
\setlength{\abovedisplayskip}{8pt} \setlength{\abovedisplayshortskip}{8pt}
\setlist{nosep}
\setlength{\parskip}{0.1cm}
\setlength{\parindent}{1em}

\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{definition}{Definition}[section]
\DeclareMathOperator*{\argmax}{argmax}
\newcommand{\R}{\ensuremath{\mathbb{R}}}

\pgfkeys{
    /tr/rowfilter/.style 2 args={
        /pgfplots/x filter/.append code={
            \edef\arga{\thisrow{#1}}
            \edef\argb{#2}
            \ifx\arga\argb
            \else
                \def\pgfmathresult{}
            \fi
        }
    }
}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\NewDocumentCommand{\normL}{ s O{} m }{%
  \IfBooleanTF{#1}{\norm*{#3}}{\norm[#2]{#3}}_{L_1}%
}

\title{Responses to the reviews of ACL rolling review paper \#162:
  ``Latent Group Dropout for Multilingual and Multidomain Machine Translation''
}
\author{}
\date{}

\begin{document}
\maketitle

\textcolor{blue}{We thank the meta-reviewer and reviewers for sharing their suggestions which will help us improve the clarity of our paper and strengthen the claim regarding the efficacy of our method.}

\section*{Meta Review Tdeo}

\subsubsection*{Meta-review:}

This paper proposes a method for parameter sharing in multitask machine translation. For a given language or domain, the model learns which groups of nodes to activate in order to optimize the multitask loss.

\subsubsection*{Summary Of Reasons To Publish:}

The reviewers mostly agree that this is an interesting, well-presented method. Multitask training is very relevant for MT, and this method achieves comparable performance to an adapter-based method with fewer parameters.

\subsubsection*{Summary Of Suggested Revisions:}

From the reviews and the ensuing discussion, it seems that the paper would benefit from comparison to more baselines. In particular, comparisons to the paper highlighted by iMF4, or to other methods already mentioned in the paper: (Li et al., (2020), Gong et al., (2021a, b)). Some reviewers mention the need for more ablations and a better understanding of hyper-parameter sensitivity.

\paragraph{}\textbf{Overall Assessment}: 3 = There are major points that may be revised
\paragraph{}\textbf{Suggested Venues:} WMT.
\paragraph{}\textbf{Ethical Concerns:} None.

\subsubsection*{Rebuttal}
{
  \color{blue}
  
As a follow up to these comments, the paper has been revised to clarify the main claims, the description of the method and presentation of experimental results with new graphs. We have also included a comparison with a new baseline suggested by Reviewer iMf4, and an analysis of the impact of meta parameters.

However, we would also like to outline the fact that our contribution, in this paper, is not to outperform the best published state-of-the-art results for multilingual / multidomain MT. Instead, our goal is to present a novel, mathematically sound, methodology to automatically learn shared nodes in multi-task (multi-domain, multi-lingual) systems and to show (a) that these new patterns yield performance that are better that alternative approaches for sharing / unsharing subpart of the computation graph accross tasks and also compare favorably in terms of computational cost; (b) that these patterns actually capture similarities between tasks. We have therefore planned our experiments, comparisons and analyses insofar as they were relevant to illustrate these aspects. This has notably motivated our choice the adapter model, which has been used in many prior studies, as a reasonable point of comparison. We however respectfully disagree on the fact that unpublished papers from mid 2021 cited in the reviews should be the ``baseline'' for reporting experiments in november 2021.

We discuss each review below, and explain the changes that have been made in response to each reviewers' suggestion.
}

% \todo{Make this contribution} Finally, we hope the reviewer change the view on the objective of our paper, reconsider the solid mathematical foundation of our method supported by an explicit implementation explained in the Appendix, the improvement with high margin in multi-domain, the reduced computational cost in the inference compared to the Adapters, and the novelty in the use of node-masking instead of weight-masking as in previous work such as in Lin et al.
% }

\section*{Reviewer iMf4}
\subsubsection*{Paper Summary:}

This paper considers multidomain and multilingual NMT as multi-task learning and proposed a method to automatically determine task-specific and task-agnostic parameters in the neural networks without explicitly defining or adding any additional layers. learning task dependent sub-networks. They formulate such problem as latent dropout mask learning in which the parameters in each transformer layer are divided into multiple groups and learn to select the k active groups for each task so that the multitask loss is minimized. Experiments in multidomain NMT and multilingual NMT show that their proposed methods achieve better average BLEU scores than the vanilla transformer and adapter methods.

\subsubsection*{Summary of Strengths:}

\begin{itemize}
\item The problem setting is interesting and well-formulated.
\end{itemize}

\subsubsection*{Summary of Weaknesses:}

\begin{itemize}
\item Missing citation and baseline: Learning task-specific and task-agnostic parameters in multitask learning for NMT has been proposed in Lin et al.2021. In this work, they learn a mask for each neuron instead of group of neurons. You should compare the proposed LaMGD method with this baseline.
\item Lack of ablation study on the dropout group selection. What if the groups are selected randomly or using some predefined heuristics, e.g. first 2 groups are shared across tasks, some other groups for language family, or a single language.

\item You should have an analysis on the effect of $k$ which potentially controls the level of sharing between tasks.

\item In table 5, although the average BLEU scores of LaMGD is higher, most of the results are not statistically significant.
  \end{itemize}

\subsubsection*{Comments, Suggestions And Typos:}

Table 7 can be improved in presentation by using heatmaps.

\textbf{Overall Assessment:} 2

\textbf{Confidence:} 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.

\textbf{Best Paper:} No

\textbf{Replicability:} 2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.

\textbf{Datasets:} 1 = No usable datasets submitted.

\textbf{Software:} 2 = Documentary: The new software will be useful to study or replicate the reported research, although for other purposes it may have limited interest or limited usability. (Still a positive rating)

\textbf{Author Identity Guess:} 1 = I do not have even an educated guess about author identity.

\subsubsection*{Rebuttal}
{\color{blue}%
We thank the reviewer for suggesting additional relevant work. We cite and discuss it in the revised version in section 5.\done\Todo{Check this}

For the sake of the clarification of our approach, we would like to point out several misunderstandings in the summary made by Reviewer iMf4:
\begin{itemize}
\item Our approach does not aim to find task-specific and task-agnostic \emph{parameters} in the neural network: there is no notion of domain-specific or domain-agnostic parameters in the approach we present. The objective of our algorithm is to optimize the assignment \emph{of a sub-group of "nodes"}, at the output of each intermediate layer, to each task.
\item Our approach does not mask the sub-parts of each weight matrix, but instead it masks a fixed number of sub-groups of elements of the output of each intermediate layer.
\end{itemize}
We have tried to improve our presentation of our method in the revised version of the paper to avoid further misunderstandings (see lines 92-95, lines 115-119).\done\todo{Change presentation, give line numbers}

We would like to answer the other concerns of the reviewer as follows. \done\todo{CLAIM : baseline + parameter sharing + resultats CDR}
\begin{itemize} 
\item Regarding baselines, we generally refer to the rebuttal to the meta review. Note that according to your suggestion, we have added new multidomain experiments with a heuristic definition of blocks. See the updated Table $3$.\done\Todo{Add table}
\item We have now included an analysis on the effect of $k$ on the performance in the revised version (see new section 4.4)
\item The multilingual experiments use two groups of $8$ languages, which are all low-resourced; this might lower the significance of the improvement as observed in the paper of Lin et al. 21 (only +0.7 for low resource languages). Our method significantly improves the less resources languages among these groups.
\item As suggested, we are now using heatmaps instead of Table 7.
\item Statistical confidence scores are also reported in Section 3.2. \done\todo{Add significancy tests}
\end{itemize}
}

\section*{Reviewer 681g}

\subsubsection*{Paper Summary:}

This paper proposes a routing strategy for multitask machine translation. For a given language or domain, only a fixed amount of nodes is activated at each layer and the model learns which nodes should be activated. They experiment on both multilingual and multidomain settings, and the experimental results demonstrate that they can achieve comparable and sometimes marginally better performance than adapter-based methods with fewer parameters.

\subsubsection*{Summary of Strengths:}
\begin{itemize}
\item The general idea is intuitive and makes sense, and the paper proposes a technically sound way of implementing it.
\item The empirical results are good as they can achieve comparable performance with an adapter-based method with fewer parameters.
\item The paper is well-organized.
\end{itemize}

\subsubsection*{Summary Of Weaknesses:}

\begin{itemize}
\item There is little analysis of their methods. Sensitivity analysis of their hyper-parameters $(n_d,k,\tau)$, some qualitative analysis are required.
\item Requiring partitioning nodes into groups and a fixed number of nodes to be activated at each layer makes their method less flexible.
\item More baselines should be included (e.g. Li et al., (2020), Gong et al., (2021a, b) as mentioned in the paper).
\end{itemize}

\subsubsection*{Comments, Suggestions And Typos:}
It'd be good to also include a mixture-of-experts baseline.

\textbf{Overall Assessment:} 3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.

\textbf{Confidence:} 4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.

\textbf{Best Paper}: No

\textbf{Replicability:} 3 = They could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined, and/or the training/evaluation data are not widely available.

\textbf{Datasets:} 1 = No usable datasets submitted.

\textbf{Software:} 1 = No usable software released.

\textbf{Author Identity Guess:} 1 = I do not have even an educated guess about author identity.

\subsubsection*{Rebuttal}

{\color{blue}
We thank the reviewer for their suggestest improvements to this work. We would like to answer the reviewer's concerns as follows.

\begin{itemize}
\item % We did admit our lack of analysis on the hyperparameters in the conclusion of our paper.
  We have now collected more data for better analyzing the effect of hyper-parameters on the performance of our method. These results in our revised version (section 4.4).
  
\item We agree with the reviewer that our method requires to set the number of blocks a priori: for us, this is just one more hyper-parameter on top of the many parameters in the Transformer (depth, number of heads, size of representations, etc) and should not be regarded as a lack of flexibility. Our analysis show that in our experimental conditions, the number of groups impacts the performance, which seem to quickly plateau as the number of blocks increases.
  
\item Regarding alternative approaches that have been recently proposed, we are aware of these works and they are duly discussed in the related work section. However, as explained in our general rebuttal above, we do not consider that comparing what we try to do with these would change any of our main claims in any way.\done\Todo{Reponse √† tous les reviewers}
\end{itemize}

We agree that a mixture of experts model would also provide yet another possible alternative baseline. Note however that mixture of experts are much costly to train, much more than our method, as we would need add new sets of parameters for each expert. Furthermore, we do not see that the outcome of such comparison would allow us to revise or amend or main claims.
}
\section*{Reviewer 3koi}

\subsubsection*{Paper Summary:}

This paper proposes a latent-variable model based on a variational probabilistic modeling framework for conditional compute in multi-domain and multilingual machine translation. They introduce latent variables which are learned during model training. These latent variables decide which sub-network of the model is selected for each task / language pair. The results on multi-domain machine translation show promise compared to vanilla Transformer and Adapter baselines. The results on multi-lingual machine translation are somewhat mixed with modest improvements in 3 settings and worse accuracy in 1 setting. The paper includes some interesting analysis of the models as well. On the other hand, some important ablations/hyper-parameter tuning experiments are missing and it is unclear how statistically significant the results on multilingual translation are.

\subsubsection*{Summary of Strengths:}

\begin{itemize}
\item This work proposes a technique to learn which subnetworks to use for which task / language pair based on learning from the data.
\item The proposed probabilistic framework is mathematically sound and explained clearly and completely.
\item There are experiments compared with decent baselines and multiple benchmarks and there has been a good attempt to evaluate the new technique well.
\item The analysis showing that accuracy of Adapter baselines declines on a domain when that domain is divided into two pseudo-domains, where as the accuracy on this proposed framework remains the same motivates the reason of using the proposed technique very well
\item A nice advantage of this approach is that it doesn‚Äôt need additional parameters to the added to the baseline Transformer model. There are some additional hyper-parameters to tune correctly per setup though.
\item There is also analysis showing whether the learned subnetworks for related domains/languages overlap significantly.

\end{itemize}

\subsubsection*{Summary of Weaknesses:}

\begin{itemize}
\item It is unclear how the latent variables that decide the subnetworks to be used per task/domain are initialized at the start of model training.
\item While the authors mention leaving this to future work, the study of how many groups to divide each layer into and how many groups to dropout per layer is missing.
\item The ablation about the effect of tuning temperature parameter $\tau$ is missing.
\item It is unclear whether the baselines have been allowed to train all the way till convergence. The multi-domain baseline model is trained for 20K updates, while the proposed model is trained for 300K updates. This looks weird.
\item Similarly, but to a lesser extent, the multilingual MT baselines are trained for 40K/(40K+5K) updates. The proposed model is trained for 50K updates. So it is unclear if the baselines have been given enough chance to convergence well.
\item It is unclear how statistically significant the improvements or decline in accuracy in the 4 setups for multilingual MT are.
\end{itemize}

\subsubsection*{Comments, Suggestions and Typos:}
[nice to have] comparison to Adapter baseline with the same number of parameters and approximate FLOPs per update as the Transformer baseline and the proposed approach would be useful.
While the proposed approach and results are quite interesting, there is some need to qualify some of the claims in the Introduction and Conclusion such as ‚Äúvery little extra computational cost‚Äù (it looks like training cost increases by 33$\%$); ‚Äúsignificant gains of this method with respect to baselines‚Äù (the gains are great in the multi-domain benchmark, but modest in the multilingual translation benchmark).

\textbf{Overall Assessment:} 3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.

\textbf{Confidence:} 3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design.

\textbf{Best Paper:} No

\textbf{Replicability:} 2 = They would be hard pressed to reproduce the results: The contribution depends on data that are simply not available outside the author's institution or consortium and/or not enough details are provided.

\textbf{Datasets:} 1 = No usable datasets submitted.

\textbf{Software:} 1 = No usable software released.

\textbf{Author Identity Guess:} 1 = I do not have even an educated guess about author identity.

\subsubsection*{Rebuttal}
{\color{blue}%
We thank the reviewer for their suggestest improvements to this work. We would like to answer the concerns of the reviewer as follows.

\begin{itemize}
\item We initialize the probabilistic distribution of the latent variables as a uniform distribution. This is clarified in the revised text in Section~3.1.4.\done\Todo{Check clarified which line}
\item The revised version includes a discussion regarding $k$ and how it affects the performance of our approach (section 4.4).
\item As reported in many works using the Gumbel reparameterization trick, the effect of $\tau$ is relatively insignificant. This is discussed in the revised version in Section 3.1.3. \done\Todo{which section}Therefore, we focused more on selecting $k$. \done\Todo{Add references}
\item Because we would like to have the same \emph{total amount of training iterations between Adapter-based and LaMGD}, we extend the training of LaMGD to 300K. The convergence of the standard Transformer is achieved before 200K as its validation curve became flat near the 200K-th iteration. These has been clarified in the paper in Section~3.1.1. \done\Todo{Which section ?} 
\item The same answer applies to the reviewer's concern regarding the multilingual experiments.
\item As pointed out\done\todo{Where ?} in Table~5, significant gains are observed in several very low-resourced languages, not globally. We have checked that these differences were significant. This might be explained by the fact that low-resourced languages benefit most from the sharing nodes with a related well-resourced languages, and are also less affected by the inference of unrelated languages.
\end{itemize}

Our claim on the advantage in computational cost wrt. Adapters relies on the inference stage while there are no extra parameters added to the standard Transformer compared to Adapter-based methods. At the same time, the training time will be more than usual because of the joint optimization of the network's organization and the parameters. \done\todo{Not so great. Can we say more ? There is a nice blog post on FLOPS here: \url{%
https://medium.com/@dzmitrybahdanau/the-flops-calculus-of-language-model-training-3b19c1f025e4
}%
}
}%

\end{document}