% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%algo packages
\usepackage{algorithm}
\usepackage{algorithmic}

%color packages
\usepackage{xcolor}
\colorlet{shadecolor}{orange!15}
\usepackage{color,soul}
\usepackage{soulutf8}
\definecolor{bordeau}{rgb}{0.3515625,0,0.234375}

% position of figure
\usepackage[absolute,overlay]{textpos}

%graphic packages
\usepackage{graphicx}
\graphicspath{ {figures/} }
\usepackage{wrapfig}
\usepackage{float} 
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{pgfplotstable}

% table packages
\usepackage{array}
\usepackage{multirow,booktabs}
\usepackage{multicol}
\usepackage{tabularx}

% font package
\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage{textcomp}
%\usepackage{times}
%\usepackage{lmodern}
\usepackage{mathptmx}

% equation packages
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{mathrsfs}


\usepackage{latexsym}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{longtable}
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage[draft]{todo}
\usepackage[normalem]{ulem}
\usepackage{xspace}
\usepackage{amsmath,amsfonts,amssymb}

\newcommand{\fyTodo}[1]{\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyTodostar}[1]{\Todo*[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyDone}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\fyFuture}[1]{\done[FY]\Todo[FY:]{\textcolor{red}{#1}}}
\newcommand{\fyDonestar}[1]{\done[FY]\Todo[FY:]{\textcolor{orange}{#1}}}
\newcommand{\jcTodo}[1]{\Todo[JC:]{\textcolor{red}{#1}}}
\newcommand{\jcDone}[1]{\done[JC]\Todo[JC:]{\textcolor{red}{#1}}}
\newcommand{\revision}[1]{\textcolor{black}{#1}}
\newcommand{\revisiondone}[1]{\textcolor{black}{#1}}
\newcommand{\revisiondel}[1]{}
\newcommand{\src}{\ensuremath{\mathbf{f}}} % source sentence
\newcommand{\trg}{\ensuremath{\mathbf{e}}} % target sentence
\newcommand{\domain}[1]{\texttt{\textsc{#1}}}
\newcommand{\system}[1]{\texttt{{#1}}}

\title{Frustratingly easy multi-domain, multilingual translation with latent multi-task group dropout}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
Multi-domain and multilingual translation can be cast under multi-task learning in which each domain or language pair is considered a task. Sharing most of the parameters of the model causes a catastrophic interference between unrelated tasks. In this work, we propose equivalently grouping the nodes of each layer and dropping a portion of these groups. Via the task-dependent dropout mechanism, we assign different sparse sub-networks to different tasks. We demonstrate a significant improvement in multi-domain and multilingual translation. Furthermore, through an ablation study, we show that the learned dropping patterns are similar in related tasks allowing the knowledge transferring between these tasks.

\end{abstract}

\section{Introduction}
Multi-domain translation and multilingual translation develop one model for translations in multi-domain and multiple language pairs respectively. These paradigms are motivated by the compactness of the translation model and the hypothetical positive knowledge transfer between similar domains \citep{Pham21revisiting} and between languages in the same family \citep{Tan19multilingual}. 

The method we developed here mitigate the catastrophic interference occurring in training a neural machine translation (NMT) model for multiple domains or multiple language pairs. 

\section{Multi-task group dropout}
We define a group dropout for a domain $d$ as follow
\begin{equation}
\begin{array}{rcl}
\tilde{h}^l &=& h^l \odot r_l^d ,\\
r_l^d \in \{ 0,1 \}^{d_k}, \\
h^{l+1} &=& \text{LAYER}^{l+1}(h^l) ,\\
l & \in & [0,\cdots,L] ,\\
\end{array}
\end{equation}
where $L$ is number of Transformer layers of our NMT model, the binary vector $r_l^d$ is a dropping mask. The dropping mask $r_l^d$ of $l^{th}$ layer and domain $d$ is defined as follow
\begin{equation}
\begin{array}{rcl}
r_l^d(i) &=& \begin{cases}
      1, & \text{if}\ p \times \frac{d_k}{n_p} \leqslant i < (p+1) \times \frac{d_k}{n_p} \\
      & \text{AND} \  m_l^d(p) \text{\small ==} 1 \\
      0, & \text{otherwise},
    \end{cases} \\
p & \in & \{0,\dots,n_p\text{-}1 \}, \\
d & \in & \{0,\dots,n_d\text{-}1 \}, \\
i & \in & \{0,\dots,d_k\text{-}1 \}, \\
m_l^d & \in & \{0,1\}^{n_p} \ \forall d,l \, \\
\mid m_l^d \mid_{L_1} & = & k , \\
\Delta^{n_p}_k & = & \{ m \in \{0,1\}^{n_p} | \mid m \mid_{L_1} = k \},
\end{array}
\end{equation}
where $n_p$ is the number of dropout groups; $n_d$ is the number of domains; $d_k$ is the size of Transformer layers; $k$ is the number of retained groups at each layer; $m_l^d$ is a binary vector indicating which group of the nodes of the $l^{th}$ layer is dropped in the sub-network modeling domain $d$. The group $p$ is retained for domain $d$ if $m_l^d(p)\text{\small ==}1$ and is dropped if $m_l^d(p)\text{\small ==}0$. We also denote $\Delta^{n_p}_k$ the set of all admissible dropping masks. 

We incorporate the latent variables $m_{l\in[0,\cdots,L]}^d$ into the log-likelihood of our model as below. For the sake of brevity, we denote $m_l^d$ the set of latent variables $m_{l\in[0,\cdots,L]}^d$
\begin{equation}
\hspace{-1.5em}
\begin{array}{rcl}
\text{log} P(y|x,\theta,d) & \text{=} & \text{log} P(y, m_l^d |x,\theta,d) \text{-} \text{log} P(m_l^d|y, x,\theta,d) \\
& \text{=} & \mathbb{E}_{P(m_l^d | \Phi_l^d)} \big[ \text{log} P(y, m_l^d | x,\theta,d) \text{-} \text{log} P(m_l^d | \Phi_l^d) \big] \\
& \text{+} & KL(P(m_l^d | \Phi_l^d) \parallel P(m_l^d|y, x,\theta,d)) \\
& \geq & \mathbb{E}_{P(m_l^d | \Phi_l^d)} \big[ \text{log} P(y, m_l^d | x,\theta,d) \text{-} \text{log} P(m_l^d | \Phi_l^d) \big]  \\
& \text{=} & \mathbb{E}_{P(m_l^d | \Phi_l^d)} \big[ \text{log} P(y | x,\theta,m_l^d, d) \big] \\
& \text{-} & KL \big( P(m_l^d | \Phi_l^d) \parallel P(m_l^d | x,\theta,d) \big) \\
& \text{=} & \mathcal{L}(\theta, \Phi_l^d, x , y) \label{eq:lower-bound}
\end{array}
\end{equation}
our training objective will be the lowerbound, which is called the evidence lower bound (ELBO) in variational statistic \citep{Diederick14auto}, $\mathcal{L}(\theta, \Phi_l^d, x , y)$. The variational distribution of $m_l^d$, $P(m_l^d|\Phi_l^d)$, is defined as follows

\begin{align}
\hspace{-2.em}
&\Phi_d \in \mathbb{R}^{n_p} , \nonumber \\
&Ind^d \text{=}  \{ i_1, \cdots, i_k \} \nonumber \\
&\sim \text{\small Sampling\_without\_replacement} \big(\text{softmax}(\Phi_l^d) \big) , \nonumber \\
&m_l^d(i) \text{=} \mathbb{I}(i \in Ind^d) .\nonumber
\end{align}

The choice of the modeling of the latent vector $m_l^d$ is motivated by the Gumbel Top-K trick \citep{Kool19stochastic}. In addition, we suppose the prior distribution of $m_l^d$ is uniform over the admissible set $\Delta^{n_p}_k$, $P(m_l^d | x, \theta, d) \ \text{=} \ U(\Delta^{n_p}_k)$.

With our assumption over the prior distribution and our choice of the variational distribution, we express the two terms of the lower-bound \eqref{eq:lower-bound} as follows

\begin{align}
\hspace{-2.em}
T_1 &= \mathbb{E}_{P(m_l^d | \Phi_l^d)} \big[ \text{log} P(y | x,\theta,m_l^d, d)  \big] \nonumber \\
&= \mathbb{E}_{g_l^d(p) \displaystyle{\mathop{\sim}^{\text{i.i.d}}} Gumbel(0,1) \ } \big[ \text{log} P(y | x,\theta,\tilde{m}_l^d,d) \big] \nonumber
\end{align}
where
\begin{align}
\hspace{-2.em}
g_l^d & \in \mathbb{R}^{n_p} \nonumber \\
g_l^d(p) & \displaystyle{\mathop{\sim}^{\text{i.i.d}}} Gumbel(0,1) \nonumber \\
& \forall p \in \{ 0,\cdots,n_p\text{-}1 \} \nonumber \\
i_1, \cdots, i_k &= Top\_k \ \{ \ g_l^d(0) + \Phi_l^d(0), \cdots, \label{eq:top-k} \\ 
&g_l^d(n_p\text{-}1) + \Phi_l^d(n_p\text{-}1) \ \} \nonumber \\
\tilde{m}_l^d(p) &= \mathbb{I}(p \in Ind^d). \nonumber 
\end{align}
%
\begin{align}
T_2 &= - KL \big( P(m_l^d | \Phi_l^d) \parallel P(m_l^d | x,\theta,d) \big) \nonumber \\ 
	&= \mathbb{H} \big[ P(m_l^d | \Phi_l^d) \big] - \text{log}(\#\Delta^{n_p}_k) \nonumber \\ 
	&= \mathbb{H} \big[ P(i_1,\cdots,i_k | \Phi_l^d) \big] - \text{log}(\#\Delta^{n_p}_k) \nonumber \\ \
	&\geqslant \mathbb{H} \big[ P(i_1 | \Phi_l^d) \big] - \text{log}(\#\Delta^{n_p}_k) \label{eq:entropy}
\end{align}

we will prove Inequality \eqref{eq:entropy} in Appendix \ref{appendix:b}.

Now we go back to our loss function, which is the negative of the lower-bound \eqref{eq:lower-bound}, consisting of 2 terms $-T_1$ and $-T_2$. The inequality \eqref{eq:entropy} shows that the upperbound of $-T_2$ is $\text{log}(\#\Delta^{n_p}_k) - \mathbb{H}(\text{softmax}(\Phi_l^d))$. In the training, we will maximize the entropy $\mathbb{H}(\text{softmax}(\Phi_l^d))$ as a regularization over the parameters $\Phi_l^d$ of the variational distribution.

Thanks to the Gumbel Top-K trick, we can move the parameters $\Phi_l^d$ into log-likelihood function and get rid of policy gradients, which have been reported to be very unstable \citep{Diederick14auto}. However, the operator Top-K, which defines $\tilde{m}_l^d$ in Equation \eqref{eq:top-k}, is not differentiable. Therefore, we approximate this function by the Soft-Top-K function defined as follows

\begin{align}
\hat{m}_l^d(\tau) &= \displaystyle{\mathop{argmin}_{\substack{
       0 \leqslant m_i \leqslant 1 \ \forall 0 \leqslant i \leqslant n_d\text{-}1 \label{eq:soft-top-k}\\
        1^{T}.m = k
      }}} -(g_l^d+\Phi_l^d)^{T} . m - \tau H_b(m) \\
& \text{in which} \nonumber \\
H_b(m) &= - \sum_i m_i \text{log}(m_i) + (1-m_i)\text{log}(1-m_i) \nonumber 
\end{align}

In Appendix \ref{appendix:a}, we will prove that $\lim_{\tau \rightarrow 0}\hat{m}_l^d(\tau) = \tilde{m}_l^d$. Furthermore, we will also provide the computation of $\hat{m}_l^d(\tau)$ and prove that $\hat{m}_l^d(\tau)$ is a differentiable function w.r.t $\Phi_l^d$ and its differentiation is computed via implicit differentiation theorem. During the training, we approximate $\tilde{m}_l^d$ by $\hat{m}_l^d(\tau)$ by gradually decaying the hyperparameter $\tau$ to $0$. The gradient of the term $T_1$ w.r.t $\Phi_l^d$ is computed via the chain rule as follows
\begin{equation}
\frac{\partial T_1}{\Phi_l^d} = \frac{\partial T_1}{\hat{m}_l^d(\tau)} \times \frac{\hat{m}_l^d(\tau)}{\Phi_l^d}
\end{equation}
The gradient $\frac{\partial T_1}{\hat{m}_l^d(\tau)}$ is computed via autograd algorithm while $\frac{\hat{m}_l^d(\tau)}{\Phi_l^d}$ is computed via implicit differentiation, that we will explain carefully in Appendix \ref{appendix:a}.
\section{Experimental settings}
\subsection{Baselines}
\subsubsection{Multi-domain, multilingual translation}
In both cases, we examinate the performance of three following MT systems.
\begin{itemize}
	\item Transformer
	\item Adapter based Transformer
	\item Transformer with multi-task group dropout
\end{itemize}
For the sake of brevity, we put the detailed descriptions of these systems in Appendix \ref{appendix:c}.
\subsection{Datasets}
\subsubsection{Multi-domain translation}
We reuse the same data of a recent work of \citet{Pham21revisiting} in multi-domain translation. For the sake of brevity, we leave most of the details of data in Appendix \ref{appendix:c}.
\subsubsection{Multilingual translation}
We evaluate our model on both one-to-many (O2M) and many-to-one (M2O)
translation tasks. We evaluate models on widely used multilingual translation datasets.
\begin{itemize}
	\item TED8-Related. Following the setting of \citet{Wang20balancing}, we use a subset of translations of \citet{qi18when} between English and eight related languages.
	\item TED8-Diverse. The dataset consists of parallel sentences between English and eight diverse languages as in \citet{Wang20balancing}.
\end{itemize}
The detailed description of the datasets is provided in Appendix \ref{appendix:c}.
\section{Results and ablation studies}
\subsection{Multi-domain translation}
According to Table \ref{tab:mdmt}, \system{LaMGD} shows a significant improvement (+2.78) over the generic system \system{Transformer} with zero extra parameters. Moreover, \system{LaMGD} achieve equivalent performance in average compared to \system{Adapter}, which is carrefully finetuned with a approximately 12.4M additional parameters.
\begin{table*}[h!]
  \centering
  \begin{tabular}{|p{4cm}|*{7}{r|}} \hline
    Model / Domain & \multicolumn{1}{c|}{\domain{ med}} & \multicolumn{1}{c|}{\domain{ law}} & \multicolumn{1}{c|}{\domain{bank}} & \multicolumn{1}{c|}{\domain{talk}} & \multicolumn{1}{c|}{\domain{ it }} & \multicolumn{1}{c|}{\domain{ rel}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline 
    \system{Transformer}  \hfill{\footnotesize[65m]} & 40.3 & 59.5 & 49.8 & 36.4 & 49.0 & 80.0  & 52.5\\
    \revision{\system{Adapter}}   \hfill{\footnotesize[+12.4m]}  & 39.5 & 61.0 & 53.1 & 37.5 & 49.6 & 91.0 & 55.28 \\ 
    \system{LaMGD}   \hfill{\footnotesize[+0m]}  & 40.3 & 60.4 & 52.4 & 39.0 & 52.4 & 87.5 & 55.33 \\ 
    \hline
  \end{tabular}
  \caption{Multi-domain translation}
  \label{tab:mdmt}
\end{table*}
\subsection{Multilingual translation}
According to Table \ref{tab:multilingual}, \system{LAMGD} shows a boost of 0.42, 0.33, ,0.32 in average over the multilingual \system{Transformer} in O2M-related, M2O-related, O2M-diverse, M2O-diverse respectively. Significant gains are observed in \domain{bel}, \domain{glg} (both direction), \domain{hin} and \domain{bos} (O2M direction).
\begin{table*}[ht]
  \centering
  \begin{tabular}{|p{4cm}|*{9}{r|}} \hline
    O2M-related & \multicolumn{1}{c|}{\domain{aze}} & \multicolumn{1}{c|}{\domain{ bel}} & \multicolumn{1}{c|}{\domain{ces}} & \multicolumn{1}{c|}{\domain{glg}} & \multicolumn{1}{c|}{\domain{por}} & \multicolumn{1}{c|}{\domain{rus}} & \multicolumn{1}{c|}{\domain{slk}} & \multicolumn{1}{c|}{\domain{tur}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline 
    \system{Transformer}  \hfill{\footnotesize[91.6m]} & 4.80 &7.30&20.80&21.10&39.70&19.80&22.60&15.20&18.91 \\
    \revision{\system{Adapter}}   \hfill{\footnotesize[13m]} & & & & & & & & & \\ 
    \system{LaMGD}  \hfill{\footnotesize[+0m]}  & 5.20&9.40&20.60&22.80&39.60&19.60&22.40&15.00&19.33 \\ 
	\hline
    \hline
    M2O-related & \multicolumn{1}{c|}{\domain{aze}} & \multicolumn{1}{c|}{\domain{ bel}} & \multicolumn{1}{c|}{\domain{ces}} & \multicolumn{1}{c|}{\domain{glg}} & \multicolumn{1}{c|}{\domain{por}} & \multicolumn{1}{c|}{\domain{rus}} & \multicolumn{1}{c|}{\domain{slk}} & \multicolumn{1}{c|}{\domain{tur}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline 
    \system{Transformer}  \hfill{\footnotesize[67.8m]} &11.40&16.60&28.50&	27.10&43.70&24.60&30.30&25.60&25.98 \\
    \revision{\system{Adapter}}   \hfill{\footnotesize[13m]} & & & & & & & & & \\ 
    \system{LaMGD}   \hfill{\footnotesize[+0m]}  &11.30&17.40&28.60&28.70&	43.70&24.50&30.70&25.60&26.31 \\ 
    \hline
    \hline
    O2M-diverse & \multicolumn{1}{c|}{\domain{bos}} & \multicolumn{1}{c|}{\domain{mar}} & \multicolumn{1}{c|}{\domain{hin}} & \multicolumn{1}{c|}{\domain{mkd}} & \multicolumn{1}{c|}{\domain{ell}} & \multicolumn{1}{c|}{\domain{bul}} & \multicolumn{1}{c|}{\domain{fra}} & \multicolumn{1}{c|}{\domain{kor}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline 
    \system{Transformer}  \hfill{\footnotesize[96.9m]} & 10.20&4.0&12.70&22.20&31.80&34.00&38.30&8.30&20.19 \\
    \revision{\system{Adapter}}   \hfill{\footnotesize[13m]}  & & & & & & & & & \\ 
    \system{LaMGD}   \hfill{\footnotesize[+0m]} & & & & & & & & & \\
    \hline 
    \hline
    O2M-diverse & \multicolumn{1}{c|}{\domain{bos}} & \multicolumn{1}{c|}{\domain{mar}} & \multicolumn{1}{c|}{\domain{hin}} & \multicolumn{1}{c|}{\domain{mkd}} & \multicolumn{1}{c|}{\domain{ell}} & \multicolumn{1}{c|}{\domain{bul}} & \multicolumn{1}{c|}{\domain{fra}} & \multicolumn{1}{c|}{\domain{kor}} & \multicolumn{1}{c|}{\domain{avg}} \\ \hline 
    \system{Transformer}  \hfill{\footnotesize[70.4m]} &22.40&9.70&20.50&31.80&37.50&38.70&39.80&19.00&27.43 \\
    \revision{\system{Adapter}}   \hfill{\footnotesize[13m]}  & & & & & & & & & \\ 
    \system{LaMGD}   \hfill{\footnotesize[+0m]} &23.50&9.60&21.50&32.20&37.70&38.60&40.00&18.90&27.75 \\
    \hline
  \end{tabular}
  \caption{Multilingual translation}
  \label{tab:multilingual}
\end{table*}
\subsection{Similarity between dropping masks}

\section{Related work}
\section{Conclusions and outlook}
\bibliography{bibliography}
\bibliographystyle{acl_natbib}
\appendix
\section{Appendix A}
\label{appendix:a}
Proof of limit. Computation of $\hat{m}_l^d(\tau)$. Theorem of implicit differentiation and Proof of differentiability.
\section{Appendix B}
\label{appendix:b}
In this section, we give a simple proof of Inequality \ref{eq:entropy}. In fact, we only need to prove $\mathbb{H} \big[ P(i_1,\cdots,i_k | \Phi_l^d) \big] \geqslant \mathbb{H} \big[ P(i_1 | \Phi_l^d) \big]$. The proof is as follows
\begin{align*}
&\mathbb{H} \big[ P(i_1,\cdots,i_k | \Phi_l^d) \big] = - \mathbb{E}_{i_1,\cdots,i_k | \Phi_l^d} \big[ \text{log}P(i_1,\cdots,i_k | \Phi_l^d) \big] \\
&= -\mathbb{E}_{i_1,\cdots,i_k | \Phi_l^d} \big[ \displaystyle{\sum_{j=2}^k}\text{log}P(i_j | i_1,\cdots,j_{j-1},\Phi_l^d) +  \text{log} P(i_1 | \Phi_l^d) \big] \\
&\geqslant -\mathbb{E}_{i_1,\cdots,i_k | \Phi_l^d} \big[ \text{log} P(i_1 | \Phi_l^d) \big] \\
&= -\mathbb{E}_{i_1 | \Phi_l^d} \big[ \text{log} P(i_1 | \Phi_l^d) \big] = \mathbb{H} \big[ P(i_1 | \Phi_l^d) \big]
\end{align*}
\section{Appendix C}
\label{appendix:c}
\subsection{Multi-domain datasets}

\begin{table*}[h!]
  \centering
  \begin{tabular}{|l|ccccccc|} %*{4}{|r|}}
    \cline{2-8} 
    %\multicolumn{4}{|l|}{Vocab size - En: 30,165, Fr: 30,398}\\
    \multicolumn{1}{c|}{} & \multicolumn{1}{c}{\domain{med}} & \multicolumn{1}{c}{\domain{law}} & \multicolumn{1}{c}{\domain{bank}} & \multicolumn{1}{c}{\domain{it}} & \multicolumn{1}{c}{\domain{talk}} & \multicolumn{1}{c}{\domain{rel}} & \multicolumn{1}{c|}{\domain{news}} \\
    \hline 
    \# lines & 2609 (0.68) & 501 (0.13) & 190 (0.05) & 270 (0.07) & 160 (0.04) & 130 (0.03) & 260 (0) \\
    \# \revisiondone{tokens}  &  133 / 154  &  17.1 / 19.6 &  6.3 / 7.3 &  3.6 / 4.6 &  3.6 / 4.0 &  3.2 / 3.4 & 7.8 / 9.2   \\
    \# \revisiondone{types}  & 771 / 720 & 52.7 / 63.1 & 92.3 / 94.7 & 75.8 / 91.4 & 61.5 / 73.3 & 22.4 / 10.5 & 77.8 / 80.4 \\
    \# \revisiondone{uniq} & 700 / 640 & 20.2 / 23.7 & 42.9 / 40.1 & 44.7 / 55.7 & 20.7 / 25.6 & 7.1 / 2.1 & 31.5 / 23.1 \\
    \hline
  \end{tabular}
  \caption{Corpora statistics: number of parallel lines ($\times 10^3$) and proportion in the basic domain mixture (which does not include the \domain{news} domain), number of tokens in English and French ($\times 10^6$), number of types in English and French ($\times 10^3$), number of types that only appear in a given domain ($\times 10^3$). \domain{med} is the largest domain, containing almost 70\% of the sentences, while \domain{rel} is the smallest, with only 3\% of the data.
  }
\label{tab:Corpora-chap4}
\end{table*}

\subsection{Multilingual datasets}

\subsection{Multi-domain systems}

\subsection{Multilingual systems}

\end{document}
